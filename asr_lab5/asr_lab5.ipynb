{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - WFST operations\n",
    "\n",
    "So far we've used WFSTs mainly as a usual structure for encoding and traversing HMMs.  In this lab we'll move away from HMM acoustic modelling and look at how WFST operations can be used to avoid the need for specialised algorithms in speech and language processing.  It is intended to give you insight into how these operations are used to construct HMMs encapsulating langauge model, pronunciation and acoustic modelling assumptions &ndash; the so-called \"HCLG\" WFST.\n",
    "\n",
    "This lab will focus on the lexicon transducer, $L$, and grammar transducer, $G$.\n",
    "\n",
    "We'll use some of the following operations, defined by Openfst:\n",
    "* `fst.determinize(f)` creates determinized version of `f`\n",
    "* `fst.compose(f1,f2)` composes FSTs `f1` and `f2`\n",
    "* `fst.shortestpath(f)` returns the shortest path (in terms of weight) through `f` from the start to a final state\n",
    "* `f.minimize()` creates minimized version of `f`\n",
    "* `f.project(project_output=False)` for every arc in `f`, copies the input label to the output label (or vice versa, if `project_output=True`).\n",
    "* `f.rmepsilon()` removes epsilon transitions &ndash; those arcs where both input and output labels are empty\n",
    "\n",
    "For efficiency, the compostion of `f1` and `f2` requires either the output arcs of `f1` or input arcs of `f2` to be sorted prior to `compose()` being called.  You can do this by calling `f1.arcsort(sort_type='olabel')` or `f2.arcsort(sort_type='ilabel')`.\n",
    "\n",
    "The functions above assume that `openfst_python` has been imported as `fst`.  Note that the first three functions above return a new WFST; the others modify the WFST *in place*, meaning that the original WFST is modified directly.\n",
    "\n",
    "For convenience, we've provided a python module `helper_functions` that provides the `parse_lexicon()` and `generate_symbol_tables()` from the [Lab 1 solutions](https://github.com/Ore-an/asr_lab1/blob/master/asr_lab1_solutions.ipynb).  And here is a function to generate an $L$ transducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openfst_python as fst\n",
    "from helper_functions import parse_lexicon, generate_symbol_tables\n",
    "\n",
    "lex = parse_lexicon('lexicon.txt')\n",
    "word_table, phone_table, state_table = generate_symbol_tables(lex)  # we won't use state_table in this lab\n",
    "\n",
    "def generate_L_wfst(lex):\n",
    "    \"\"\" express the lexicon in WFST form\n",
    "    \n",
    "    Args:\n",
    "        lexicon (dict): lexicon to use, created from the parse_lexicon() function\n",
    "    \n",
    "    Returns:\n",
    "        the constructed lexicon WFST\n",
    "    \n",
    "    \"\"\"\n",
    "    L = fst.Fst()\n",
    "    \n",
    "    # create a single start state\n",
    "    start_state = L.add_state()\n",
    "    L.set_start(start_state)\n",
    "    \n",
    "    for (word, pron) in lex.items():\n",
    "        \n",
    "        current_state = start_state\n",
    "        for (i,phone) in enumerate(pron):\n",
    "            next_state = L.add_state()\n",
    "            \n",
    "            if i == len(pron)-1:\n",
    "                # add word output symbol on the final arc\n",
    "                L.add_arc(current_state, fst.Arc(phone_table.find(phone), \\\n",
    "                                                 word_table.find(word), None, next_state))\n",
    "            else:\n",
    "                L.add_arc(current_state, fst.Arc(phone_table.find(phone),0, None, next_state))\n",
    "            \n",
    "            current_state = next_state\n",
    "                          \n",
    "        L.set_final(current_state)\n",
    "        \n",
    "    L.set_input_symbols(phone_table)\n",
    "    L.set_output_symbols(word_table)                      \n",
    "    \n",
    "    return L\n",
    "\n",
    "L = generate_L_wfst(lex)\n",
    "L.arcsort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exercises, here are two functions to generate linear WFSTs for an arbitary sequence of phones or words.  (Yes, they are really just variants of the same function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_phone_wfst(phone_list):\n",
    "    \n",
    "    P = fst.Fst()\n",
    "    \n",
    "    current_state = P.add_state()\n",
    "    P.set_start(current_state)\n",
    "    \n",
    "    for p in phone_list:\n",
    "        \n",
    "        next_state = P.add_state()\n",
    "        P.add_arc(current_state, fst.Arc(phone_table.find(p), phone_table.find(p), None, next_state))\n",
    "        current_state = next_state\n",
    "        \n",
    "    P.set_final(current_state)\n",
    "    P.set_input_symbols(phone_table)\n",
    "    P.set_output_symbols(phone_table)\n",
    "    return P\n",
    "    \n",
    "def generate_linear_word_wfst(word_list):\n",
    "    \n",
    "    W = fst.Fst()\n",
    "    \n",
    "    current_state = W.add_state()\n",
    "    W.set_start(current_state)\n",
    "    \n",
    "    for w in word_list:\n",
    "        \n",
    "        next_state = W.add_state()\n",
    "        W.add_arc(current_state, fst.Arc(word_table.find(w), word_table.find(w), None, next_state))\n",
    "        current_state = next_state\n",
    "        \n",
    "    W.set_final(current_state)\n",
    "    W.set_input_symbols(word_table)\n",
    "    W.set_output_symbols(word_table)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Suppose you are given a sequence of phones, in the form `['p','ih','k','t']`, and the $L$ transducer created above.  Write a function that returns the matching word from the lexicon for any given phone sequence, or else `None` if no matching word is found.   Write two functions:\n",
    "  1. That works for $L$ as provided by the code above\n",
    "  2. That works only on a determinized version of $L$ &ndash; and test it on the output of `fst.determinize(L)`\n",
    "  \n",
    " This should enable you to see why determinization is a very useful WFST operation!\n",
    "  \n",
    "1. WFST composition allows you to achieve the same result much more easily.  Create a linear WFST, $P$, correspondinng to a string of phones, and compute $P \\circ L$.  Then use the projection and epsilon removal operations to display just the matching word.  \n",
    "\n",
    "3. Modify your lexicon WFST slightly to allow an arbitrary string of phones to be \"decoded\" to a sequence of words from the lexicon, using composition.  Try it with `['p','eh','k','ah','v','p','iy','t','er']`.  \n",
    "\n",
    "1. Now solve the reverse problem: create a word-sequence WFST, $W$, and use composition to expand it into a sequence of phones.\n",
    "\n",
    "1. (Harder) Use WFST composition to implement a \"predictive text\"-style algorithm, that, given a partial phone sequence such as `['p']` or `['p','ih']`, returns a WFST giving all matching words.  You'll need to make some special modifications to $P$ or $L$, or both. On a determinized $L$ transducer this is a highly efficient way of solving this problem.  \n",
    "\n",
    "1. Another advantage of WFST composition to solve these kind of problems are that it is easy to encode uncertainty in the input (a bit like in real ASR).  For example, consider this WFST, in which the multiple arcs denote alternative phone transcriptions from the acoustic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alt_phone_wfst(phone_alternatives):\n",
    "    \n",
    "    P = fst.Fst()\n",
    "    \n",
    "    current_state = P.add_state()\n",
    "    P.set_start(current_state)\n",
    "    \n",
    "    for alt in phone_alternatives:\n",
    "        \n",
    "        next_state = P.add_state()\n",
    "        for p in alt:\n",
    "            if p=='*':\n",
    "                P.set_final(current_state)\n",
    "            else:\n",
    "                P.add_arc(current_state, fst.Arc(phone_table.find(p), phone_table.find(p), None, next_state))\n",
    "        current_state = next_state\n",
    "        \n",
    "    P.set_final(current_state)\n",
    "    P.set_input_symbols(phone_table)\n",
    "    P.set_output_symbols(phone_table)\n",
    "    return P    \n",
    "    \n",
    "P = create_alt_phone_wfst([['p'],['ay'],['p'],['er'],['p'],['eh','ih'],['k'],['t','<eps>'],['ah','<eps>'],['l','v','*'],['d','*']])\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Again, perform composition with your $L$ from Question 3, and observe the result.  (Notice particularly what happens to the `<eps>` transitions during composition.  \n",
    "  \n",
    "7. We could have added weights to the arcs of the WFST above to describe the probability of the phone alternatives given by the acoustic model &ndash; this would have enabled you to find the most likely sequence of words.  Without this information, let's instead use a $G$ WFST to find the most likely sequence.  Let's assume that a word sequence taken from the passage \"peter piper picked a peck of pickled peppers\" is most likely.  Design a $G$ WFST that accepts any sequence of words from the lexicon, but adds a cost of 1.0 to any word transition not in the passage.  Given $G$, use composition to recover the most likely word sequence from the uncertain $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
